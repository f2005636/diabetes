{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxMV0DNjRppJ"
   },
   "source": [
    "### 1. importing dataset\n",
    "* separating out x and y\n",
    "* adding monotonic constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = pd.read_csv('02 data split 0.csv')\n",
    "s1 = pd.read_csv('02 data split 1.csv')\n",
    "s2 = pd.read_csv('02 data split 2.csv')\n",
    "mc = [0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1]\n",
    "ic = ['race','gender','age','admission_type_id','admission_source_id','metformin','nateglinide',\n",
    "      'glipizide','glyburide','pioglitazone','insulin','diabetesMed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Light GBM\n",
    "* identifying optimal hyper-parameters\n",
    "* performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:10<00:00, 17.72s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:23<00:00, 20.88s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:32<00:00, 23.07s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:32<00:00, 23.20s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:34<00:00, 23.53s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:33<00:00, 23.42s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:34<00:00, 23.59s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:34<00:00, 23.70s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:34<00:00, 23.69s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:35<00:00, 23.93s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:38<00:00, 24.59s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:38<00:00, 24.60s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:40<00:00, 25.07s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:40<00:00, 25.09s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:39<00:00, 24.96s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:40<00:00, 25.07s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:40<00:00, 25.09s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:41<00:00, 25.32s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:41<00:00, 25.34s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:40<00:00, 25.01s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:39<00:00, 24.80s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:39<00:00, 24.83s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:39<00:00, 24.82s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:39<00:00, 24.80s/it]\n",
      "100%|\u001b[34m██████████\u001b[0m| 4/4 [01:40<00:00, 25.00s/it]\n"
     ]
    }
   ],
   "source": [
    "objective_l = []\n",
    "metric_l = []\n",
    "boosting_type = []\n",
    "lambda_l1_l = []\n",
    "lambda_l2_l = []\n",
    "learning_rate_l = []\n",
    "max_depth_l = []\n",
    "min_data_in_leaf_l = []\n",
    "n_estimators_l = []\n",
    "split_l = []\n",
    "auc_train_l = []\n",
    "f1_train_l = []\n",
    "auc_test_l = []\n",
    "f1_test_l = []\n",
    "\n",
    "for lambda_l1_i in [0,0.1,0.2,0.5,1]:\n",
    "    for lambda_l2_i in [0,0.1,0.2,0.5,1]:\n",
    "        for learning_rate_i in tqdm([0.1,0.2,0.5,1], colour='blue'):\n",
    "            for max_depth_i in [2,5,10,20,50]:\n",
    "                for min_data_in_leaf_i in [10,20,50,100]:\n",
    "                    for n_estimators_i in [10,20,50,100]:\n",
    "                        for i in [0,1,2]:\n",
    "\n",
    "                            model = lgb.LGBMClassifier(objective='binary',metric='auc',boosting_type='gbdt',\n",
    "                                                    monotone_constraints=mc,categorical_feature=ic,missing='?',\n",
    "                                                    lambda_l1=lambda_l1_i,lambda_l2=lambda_l2_i,learning_rate=learning_rate_i,\n",
    "                                                    max_depth=max_depth_i,min_data_in_leaf=min_data_in_leaf_i,n_estimators=n_estimators_i,\n",
    "                                                    random_state=0,verbose=-1)\n",
    "                            objective_l.append('binary')\n",
    "                            metric_l.append('auc')\n",
    "                            boosting_type.append('gbdt')\n",
    "                            lambda_l1_l.append(lambda_l1_i)\n",
    "                            lambda_l2_l.append(lambda_l2_i)\n",
    "                            learning_rate_l.append(learning_rate_i)\n",
    "                            max_depth_l.append(max_depth_i)\n",
    "                            min_data_in_leaf_l.append(min_data_in_leaf_i)\n",
    "                            n_estimators_l.append(n_estimators_i)\n",
    "                            split_l.append(i)\n",
    "\n",
    "                            if i==0:\n",
    "                                train = pd.concat([s1,s2])\n",
    "                                test = s0\n",
    "                            if i==1:\n",
    "                                train = pd.concat([s2,s0])\n",
    "                                test = s1\n",
    "                            if i==2:\n",
    "                                train = pd.concat([s0,s1])\n",
    "                                test = s2\n",
    "                            for j in ic:\n",
    "                                train[j] = train[j].astype('category')\n",
    "                                test[j] = test[j].astype('category')\n",
    "                                \n",
    "                            y = train['readmitted']\n",
    "                            x = train.drop(['readmitted'], axis=1)\n",
    "                            model.fit(x,y)\n",
    "                            pred2 = []\n",
    "                            pred1 = model.predict_proba(x)[:,1]\n",
    "                            a1 = roc_auc_score(y,pred1)\n",
    "                            for j in pred1:\n",
    "                                if j > 0.30: pred2.append(1)\n",
    "                                else: pred2.append(0)\n",
    "                            c1 = confusion_matrix(y,pred2)\n",
    "                            p = c1[1][1] / (c1[0][1]+c1[1][1])\n",
    "                            r = c1[1][1] / (c1[1][0]+c1[1][1])\n",
    "                            f1 = (2*p*r) / (p+r)\n",
    "                            auc_train_l.append(a1)\n",
    "                            f1_train_l.append(f1)\n",
    "                            \n",
    "                            y = test['readmitted']\n",
    "                            x = test.drop(['readmitted'], axis=1)\n",
    "                            pred2 = []\n",
    "                            pred1 = model.predict_proba(x)[:,1]\n",
    "                            a1 = roc_auc_score(y,pred1)\n",
    "                            for j in pred1:\n",
    "                                if j > 0.30: pred2.append(1)\n",
    "                                else: pred2.append(0)\n",
    "                            c1 = confusion_matrix(y,pred2)\n",
    "                            p = c1[1][1] / (c1[0][1]+c1[1][1])\n",
    "                            r = c1[1][1] / (c1[1][0]+c1[1][1])\n",
    "                            f1 = (2*p*r) / (p+r)\n",
    "                            auc_test_l.append(a1)\n",
    "                            f1_test_l.append(f1)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. test\n",
    "* adding monotonic constraints\n",
    "* performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'objective':objective_l, 'metric':metric_l, 'lambda_l1': lambda_l1_l, 'lambda_l2':lambda_l2_l,\n",
    "                   'learning_rate':learning_rate_l, 'max_depth':max_depth_l, 'min_data_in_leaf':min_data_in_leaf_l,\n",
    "                   'n_estimators':n_estimators_l, 'split':split_l, 'auc_train':auc_train_l, 'f1_train':f1_train_l,\n",
    "                   'auc_test':auc_test_l, 'f1_test':f1_test_l})\n",
    "gb = df.groupby(['objective','metric','lambda_l1','lambda_l2','learning_rate','max_depth','min_data_in_leaf',\n",
    "                 'n_estimators']).agg({'auc_train':'mean','f1_train':'mean','auc_test':'mean','f1_test':'mean'}).reset_index()\n",
    "gb.to_csv('03 hyper-parameters.csv', index=False)\n",
    "gb.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
